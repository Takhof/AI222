# ğŸœ Ramen Retriever AI

æ—¥æœ¬èªBERTãƒ™ãƒ¼ã‚¹ã§æ§‹ç¯‰ã—ãŸã€ãƒ©ãƒ¼ãƒ¡ãƒ³å°‚é–€ã®è³ªå•å¿œç­”ãƒ¢ãƒ‡ãƒ«ï¼  
è³ªå•ã«å¯¾ã—ã¦ã€ã‚‚ã£ã¨ã‚‚é©åˆ‡ãªãƒ©ãƒ¼ãƒ¡ãƒ³æƒ…å ±ã‚’é«˜ç²¾åº¦ã«ãƒãƒƒãƒãƒ³ã‚°ã™ã‚‹AIã§ã™âœ¨  

---

## ğŸš€ æ©Ÿèƒ½æ¦‚è¦

- ğŸ¤– [`cl-tohoku/bert-base-japanese`](https://huggingface.co/cl-tohoku/bert-base-japanese) ã«ã‚ˆã‚‹BERTãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨  
- ğŸ’¡ è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’æ­£è§£ï¼ˆ1ï¼‰ãƒ»ä¸æ­£è§£ï¼ˆ0ï¼‰ã¨ã—ã¦åˆ†é¡ã™ã‚‹ãƒã‚¤ãƒŠãƒªåˆ†é¡ãƒ¢ãƒ‡ãƒ«  
- ğŸ”€ è¿½åŠ ã®Q&Aã§ç°¡å˜ã«å†å­¦ç¿’ãƒ»Fine-tuningå¯èƒ½  
- ğŸ§  Streamlitã‚„APIã¸ã®å¿œç”¨ã‚‚ã‚«ãƒ³ã‚¿ãƒ³ï¼  

---

## ğŸ“œ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

```
ramen-retriever-ai/
â”œâ”€â”€ model_creator.py         # æœ€åˆã®ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ model_finetune.py        # è¿½åŠ å­¦ç¿’ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ retriever_qa.json        # è³ªå•ãƒ»æ­£è§£ãƒ»èª¤ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
â”œâ”€â”€ qa_data.json             # è¿½åŠ å­¦ç¿’ç”¨ã®Q&Aãƒ‡ãƒ¼ã‚¿ï¼ˆquestion/answerå½¢å¼ï¼‰
â”œâ”€â”€ ramen_model.h5           # å­¦ç¿’æ¸ˆã¿Kerasãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ requirements.txt         # ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
â””â”€â”€ README.md                # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

---

## ğŸ› ï¸ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ–¹æ³•

### 1. Pythonç’°å¢ƒã®æº–å‚™

```bash
python -m venv venv
source venv/bin/activate  # Windowsã®æ–¹ã¯ venv\Scripts\activate
pip install -r requirements.txt
```

---

## ğŸ”ª ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆåˆå›ï¼‰

`retriever_qa.json` ã‚’ä»¥ä¸‹ã®å½¢å¼ã§ç”¨æ„ã—ã¦ãã ã•ã„ï¼š

```json
[
  {
    "question": "ãƒ©ãƒ¼ãƒ¡ãƒ³ã®ã‚¹ãƒ¼ãƒ—ã®ç¨®é¡ã¯ï¼Ÿ",
    "positive": "é†¤æ²¹ã€å‘³å™Œã€å¡©ã€ã¨ã‚“ã“ã¤ãªã©ãŒã‚ã‚Šã¾ã™ã€‚",
    "negatives": [
      "ã‚µãƒƒã‚«ãƒ¼ã¯11äººã§ã‚„ã‚Šã¾ã™ã€‚",
      "ãƒšãƒ³ã‚®ãƒ³ã¯é£›ã¹ã¾ã›ã‚“ã€‚"
    ]
  }
]
```

ãã®ã‚ã¨ä»¥ä¸‹ã‚’å®Ÿè¡Œï¼š

```bash
python model_creator.py
```

ãƒ¢ãƒ‡ãƒ«ã¯ `ramen_model.h5` ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¾ã™ã€‚

---

## ğŸ” è¿½åŠ å­¦ç¿’ï¼ˆFine-tuningï¼‰

è¿½åŠ ãƒ‡ãƒ¼ã‚¿ï¼ˆ`qa_data.json`ï¼‰ã‚’ä»¥ä¸‹ã®å½¢å¼ã§ä½œæˆï¼š

```json
[
  {
    "question": "å‘³å™Œãƒ©ãƒ¼ãƒ¡ãƒ³ã®ç‰¹å¾´ã¯ï¼Ÿ",
    "answer": "æ¿ƒåšãªå‘³å™Œã ã‚ŒãŒç‰¹å¾´ã§ã™ã€‚"
  }
]
```

ä»¥ä¸‹ã‚’å®Ÿè¡Œï¼š

```bash
python model_finetune.py
```

å†å­¦ç¿’å¾Œã®ãƒ¢ãƒ‡ãƒ«ã¯ `ramen_model_finetuned.h5` ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¾ã™ã€‚

---

## ğŸ§ª æ¨è«–ï¼ˆã‚¤ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ï¼‰

æ¨è«–ç”¨ã‚³ãƒ¼ãƒ‰ä¾‹ï¼ˆStreamlitã‚„APIã«å¿œç”¨å¯èƒ½ï¼‰ï¼š

```python
from transformers import BertTokenizer
import tensorflow as tf

tokenizer = BertTokenizer.from_pretrained("cl-tohoku/bert-base-japanese")
model = tf.keras.models.load_model("ramen_model_finetuned.h5")

def predict_score(question, answer):
    inputs = tokenizer(question, answer, padding="max_length", max_length=64, truncation=True, return_tensors="tf")
    pred = model.predict([inputs["input_ids"], inputs["attention_mask"]])
    return float(pred[0][0])
```

---

## ğŸš¼ .gitignore ã«å…¥ã‚Œã¦ãŠãã¨ä¾¿åˆ©ãªã‚‚ã®

```
__pycache__/
*.pyc
*.h5
*.keras
.env
.vscode/
.idea/
```

---

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License âœ¨
