from tensorflow import keras
from transformers import BertTokenizer
import numpy as np
from transformers import TFBertModel


# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
model = keras.models.load_model(
    r"C:\Users\takus\testramen\ramen_retriever.h5",
    custom_objects={"TFBertModel": TFBertModel}
)


# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æº–å‚™
tokenizer = BertTokenizer.from_pretrained("cl-tohoku/bert-base-japanese")

# è³ªå•
question = "ãƒ©ãƒ¼ãƒ¡ãƒ³ã®ã‚¹ãƒ¼ãƒ—ã£ã¦ã©ã†ä½œã‚‹ã®ï¼Ÿ"

# å€™è£œæ–‡ãŸã¡
candidates = [
    "ã‚¹ãƒ¼ãƒ—ã¯è±šéª¨ã‚„é¶ã‚¬ãƒ©ã‚’é•·æ™‚é–“ç…®è¾¼ã‚“ã§ä½œã‚Šã¾ã™ã€‚",
    "å‘³å™Œãƒ©ãƒ¼ãƒ¡ãƒ³ã¯åŒ—æµ·é“ã§ç”Ÿã¾ã‚ŒãŸãƒ©ãƒ¼ãƒ¡ãƒ³ã§ã™ã€‚",
    "ãƒãƒ£ãƒ¼ã‚·ãƒ¥ãƒ¼ã¯è±šãƒãƒ©è‚‰ã§ä½œã‚‹ãƒˆãƒƒãƒ”ãƒ³ã‚°ã§ã™ã€‚",
    "ã¡ã¢ã‚Œéººã¯ã‚¹ãƒ¼ãƒ—ãŒã‚ˆãçµ¡ã‚€ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚"
]

# æ¨è«–æº–å‚™
def tokenize_pair(q, a, max_len=64):
    inputs = tokenizer(q, a, padding='max_length', truncation=True, max_length=max_len, return_tensors="tf")
    return inputs['input_ids'][0], inputs['attention_mask'][0]

# å…¨å€™è£œã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
input_ids_list = []
attention_masks_list = []

for ans in candidates:
    ids, mask = tokenize_pair(question, ans)
    input_ids_list.append(ids)
    attention_masks_list.append(mask)

input_ids = np.stack(input_ids_list)
attention_masks = np.stack(attention_masks_list)

# ãƒ¢ãƒ‡ãƒ«ã«é£Ÿã¹ã•ã›ã‚‹ğŸœ
preds = model.predict([input_ids, attention_masks])

# ä¸€ç•ªã‚¹ã‚³ã‚¢ãŒé«˜ã„å€™è£œã‚’é¸ã¶
best_idx = np.argmax(preds)
print("è³ªå•:", question)
print("å€™è£œã®ä¸­ã§ã„ã¡ã°ã‚“åˆã£ã¦ã‚‹ç­”ãˆã¯â€¦ğŸ’¡")
print("ğŸ‘‰", candidates[best_idx])
print("ã‚¹ã‚³ã‚¢:", preds[best_idx][0])